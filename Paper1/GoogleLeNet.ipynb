{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GoogleLeNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V_YdD-3YScl"
      },
      "source": [
        "### GoogleLeNet를 keras를 이용해서 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU-VQmaLVtaM"
      },
      "source": [
        "from keras.models import Model, Input\r\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, GlobalAveragePooling2D, Dense, AveragePoolig2D, Flatten, Concatnate\r\n",
        "from keras.optimizers import SGD #확률적 경사 하강법 사용\r\n",
        "from keras.callbacks import Callback\r\n",
        "\r\n",
        "from keras.utils.import to_categorical #one-hot-encoding을 진행하기 위해서\r\n",
        "from keras.datasets import cifar10\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR7lJqDPxj5C"
      },
      "source": [
        "class LearningRateSchedule(keras.callbacks.Callback):\r\n",
        "  def on_epoch_end(self, epoch, logs = None):\r\n",
        "    if (epoch + 1) % 8 == 0:\r\n",
        "      lr = K.get_value(self.model.optimizer.lr)\r\n",
        "      K.set_value(self.model.optimizer.lr, lr*0.96)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRSnRy0j4VSL"
      },
      "source": [
        "#### LocalResponseNormalization\r\n",
        "```py\r\n",
        "import tensorflow as tf\r\n",
        "tf.nn.local_response_normalization(\r\n",
        "  input, depth_radius, bias, alpha, beta, name = None\r\n",
        ")\r\n",
        "```\r\n",
        "- 일반적으로 요즘은 ReLU activation function을 사용한 경우에도 batch normalization을 사용하곤 한다.\r\n",
        "- AlexNet에서 LRN을 사용한 이유는 ReLU가 양수의 방향으로는 입력의 값을 그대로 사용한다는 특성 떄문이다.\r\n",
        "  - 그렇게 되면 Convolution / Pooling layer에서 매우 놏은 하나의 픽셀값이 주변의 픽셀에 영향을 주게 될 것이다.\r\n",
        "  - 따라서 이런 부분을 방지하기 위해 **같은 위치에 있는 픽셀끼리의 정규화**를 진행하는 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmvawnTjx89P"
      },
      "source": [
        "class LocalResponseNormalization(keras.layers.Layer):\r\n",
        "  def __init__(self, n = 5, alpha = 1e-4, beta = 0.75, k = 2, **kwargs):\r\n",
        "    self.n = n\r\n",
        "    self.alpha = alpha\r\n",
        "    self.beta = beta\r\n",
        "    self.k = k\r\n",
        "    super(LocalResponseNormalization, self).__init__(**kwargs)\r\n",
        "  \r\n",
        "  def build(self, input_shape):\r\n",
        "    self.shape = input_shape\r\n",
        "    super(LocalResopnseNormalization, self).build(input_shape)\r\n",
        "  \r\n",
        "  def call(self, x):\r\n",
        "    _, r, c, f = self.shape\r\n",
        "    squared = K.square(x)\r\n",
        "    pooled = K.pool2d(squares, (self.n, self.n), strides = (1,1), padding = 'same', pool_mode = 'avg')\r\n",
        "    summed = K.sum(pooled, axis = 3, keepdims = True)\r\n",
        "    averaged = self.alpha * K.repeat_elements(summed, f, axis = 3)\r\n",
        "\r\n",
        "    denom = K.pow(self.k + averaged, self.beta)\r\n",
        "\r\n",
        "    return x/denom\r\n",
        "  \r\n",
        "  def get_output_shape(self, input_shape):\r\n",
        "    return input_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhsYcpdJr_N0"
      },
      "source": [
        "def Inception(input_tensor, filter_channels):\r\n",
        "  filter_1x1, filter_3x3_R, filter_3x3, filter_5x5_R, filter_5x5, pool_proj = filter_channels\r\n",
        "\r\n",
        "  branch_1 = Conv2D(filter_1x1, (1,1), strides = 1, padding = 'same',\r\n",
        "                    activation = 'relu', kernel_initializer = 'he_normal')(input_tensor)\r\n",
        "\r\n",
        "  branch_2 = Conv2D(filter_3x3_R, (1,1), strides = 1, padding = 'same',\r\n",
        "                    activation = 'relu', kernel_initializer = 'he_normal')(input_tensor)\r\n",
        "  branch_2 = Conv2D(filter_3x3, (3,3), strides = 1, padding = 'same', \r\n",
        "                    activation = 'relu', kernel_initializer = 'he_normal')(branch_2)\r\n",
        "\r\n",
        "  branch_3 = Conv2D(filter_5x5_R, (1,1), strides = 1, padding = 'same', \r\n",
        "                    activation = 'relu', kernel_initializer = 'he_normal')(input_tensor)\r\n",
        "  branch_3 = Conv2D(filter_5x5, (5,5), strides = 1, padding = 'same',\r\n",
        "                    activation = 'relu', kernel_initializer = 'he_normal')(branch_3)\r\n",
        "\r\n",
        "  branch_4 = MaxPooling2D((3,3), strides = 1, padding = 'same')(input_tensor)\r\n",
        "  branch_4 = Conv2D((1,1), strides = 1, padding = 'same')(branch_4)\r\n",
        "\r\n",
        "  Concat = Concatenate([branch_1, branch_2, branch_3, branch_4])\r\n",
        "\r\n",
        "  return Concat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyrXj6GMzKOm"
      },
      "source": [
        "def GoogLeNet(model_intput, classes = 10):\r\n",
        "  # Main Classifier\r\n",
        "  conv_1 = Conv2D(64, (7,7), strides = 2, padding = \"same\", activation = \"relu\")(model_input) #(112, 112, 64)\r\n",
        "  pool_1 = MaxPooling2D((3, 3), strides = 2, padding = \"same\", activation = \"relu\")(conv_1) # (56, 56, 64)\r\n",
        "  LRN_1 = LocalResponseNormalization()(pool_1) # (56, 56, 64)\r\n",
        "\r\n",
        "  conv_2 = Conv2D(64, (1, 1), strides = 1, padding = \"valid\", activation = \"relu\")(LRN_1) # (56, 56, 64)\r\n",
        "  conv_3 = Conv2D(192, (3, 3), strides = 1, padding = \"same\", activation = \"relu\")(conv_2) # (56, 56, 192)\r\n",
        "  LRN_2 = LocalResponseNormalization()(conv_3) # (56, 56, 192)\r\n",
        "  pool_2 = MaxPooling2D((3,3), strides = 2, padding = \"same\", activation = \"relu\")(LRN_2) # (28, 28, 192)\r\n",
        "\r\n",
        "  inception_3a = inception(pool_2, [64, 96, 128, 16, 32, 32]) # (28, 28, 256) \r\n",
        "  # 256 = \r\n",
        "  inception_3b = inception(inception_3a, [128, 128, 192, 32, 96, 64]) # (28, 28, 480)\r\n",
        "\r\n",
        "  pool_3 = MaxPooing2D((3, 3), strides = 2, padding = \"same\", activation = \"relu\")(inception_3b) # (14, 14, 480)\r\n",
        "\r\n",
        "  inception_4a = inception(pool_3, [192, 96, 208, 16, 48, 64]) # (14, 14, 512)\r\n",
        "  inception_4b = inception(inception_4a, [160, 112, 224, 24, 64, 64]) # (14, 14, 512)\r\n",
        "  inception_4c = inception(inception_4b, [128, 128, 256, 24, 64, 64]) # (14, 14, 512)\r\n",
        "  inception_4d = inception(inception_4c, [112, 144, 288, 32, 64, 64]) # (14, 14, 528)\r\n",
        "  inception_4e = inception(inception_4d, [256, 160, 320, 32, 128, 128]) # (14, 14, 832)\r\n",
        "\r\n",
        "  pool_4 = MaxPooling2D((3, 3), strides = 2, padding = \"same\")(inception_4e)\r\n",
        "\r\n",
        "  inception_5a = inception(pool_4, [256, 160, 320, 32, 128, 128]) # (7, 7, 832)\r\n",
        "  inception_5b = inception(inception_5a, [384, 192, 384, 48, 128, 128]) # (7, 7, 1024)\r\n",
        "\r\n",
        "  avg_pool = GlobalAveragePooling2D()(inception_5b)\r\n",
        "  dropout = Dropout(0.4)(avg_pool)\r\n",
        "\r\n",
        "  linear = Dense(1000, activation = \"relu\")(dropout)\r\n",
        "\r\n",
        "  model_output = Dense(classes, activation = \"softmax\", name = \"main_classifier\")(linear)\r\n",
        "\r\n",
        "  # Auxiliary Classifier\r\n",
        "  auxiliary_4a = AveragePooling2D((5, 5), strides = 3, padding = \"valid\")(inception_4a)\r\n",
        "  auxiliary_4a = Conv2D(128, (1,1), strides = 1, padding = \"same\")(auxiliary_4a)\r\n",
        "  auxiliary_4a = Flatten()(auxiliary_4a)\r\n",
        "  auxiliary_4a = Dense(1024, activation = \"relu\")(auxiliary_4a)\r\n",
        "  auxiliary_4a = Dropout(0.7)(auxiliary_4a)\r\n",
        "  auxiliary_4a = Dense(classes, activation = \"softmax\", name = \"auxiliary_4a\")(auxiliary_4a)\r\n",
        "\r\n",
        "  auxiliary_4d = AveragePooling2D((5, 5), strides = 3, padding = \"valid\")(inception_4d)\r\n",
        "  auxiliary_4d = Conv2D(128, (1,1), strides = 1, padding = \"same\")(auxiliary_4d)\r\n",
        "  auxiliary_4d = Flatten()(auxiliary_4d)\r\n",
        "  auxiliary_4d = Dense(1024, activation = \"relu\")(auxiliary_4d)\r\n",
        "  auxiliary_4d = Dropout(0.7)(auxiliary_4d)\r\n",
        "  auxiliary_4d = Dense(classes, activation = \"softmax\", name = \"auxiliary_4a\")(auxiliary_4d)\r\n",
        "\r\n",
        "  model = Model(model_input, [model_output, auxiliary_4a, auxiliary_4d])\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crXLou6AuhpR"
      },
      "source": [
        "input_shape = (224, 224, 3)\r\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n",
        "\r\n",
        "x_train = Upscale_Data(x_train, input_shape)/255.0\r\n",
        "x_test = Upscale_Data(x_test, input_shape)/255.0\r\n",
        "\r\n",
        "y_train = to_categorical(y_train, num_classes = 10)\r\n",
        "y_test = to_categorical(y_test, num_classes = 10)\r\n",
        "\r\n",
        "model_input = Input(shape = input_shape)\r\n",
        "\r\n",
        "model = GoogLeNet(model_input, 10)\r\n",
        "\r\n",
        "optimizer = SGD(momentum = 0.9)\r\n",
        "\r\n",
        "model.compile(optimizer, loss = {'main_classifier' : 'categorical_crossentropy', 'auxiliary_4a' : 'categorical_crossentropy',\r\n",
        "                                 'auxiliary_4d' : 'categorical_crossentropy'},\r\n",
        "              loss_weights = {'main_classifier' : 1.0, 'auxiliary_4a' : 0.3, 'auciliary_4b' : 0.3},\r\n",
        "              metrics = ['acc'])\r\n",
        "\r\n",
        "model.fit(x_train,\r\n",
        "          {'main_classifier' : y_train, 'auxiliary_4a' : y_train, 'auxiliary_4b' : y_train},\r\n",
        "          epochs = 100, batch_size = 32, callbacks = LearningRateSchedule())\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}