{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mask R-CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPCzyk7Q-fyv"
      },
      "source": [
        "### Initializations (Utils)\n",
        "#### 1. RoI allign"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbbYdDka-9JB"
      },
      "source": [
        "import torch\n",
        "from torch import nn, Tensor\n",
        "\n",
        "from torch.nn.modules.utils import _pair\n",
        "from torch.jit.annotations import BroadcastingList2\n",
        "\n",
        "from torchvision.extension import _assert_has_ops\n",
        "from ._utils import convert_boxes_to_roi_format, check_roi_boxes_shape\n",
        "\n",
        "\n",
        "def roi_align(\n",
        "    input: Tensor,\n",
        "    boxes: Tensor,\n",
        "    output_size: BroadcastingList2[int],\n",
        "    spatial_scale: float = 1.0,\n",
        "    sampling_ratio: int = -1,\n",
        "    aligned: bool = False,\n",
        ") -> Tensor:\n",
        "    \"\"\"\n",
        "    Performs Region of Interest (RoI) Align operator described in Mask R-CNN\n",
        "    Args:\n",
        "        input (Tensor[N, C, H, W]): input tensor\n",
        "            If the tensor is quantized, we expect a batch size of ``N == 1``.\n",
        "        boxes (Tensor[K, 5] or List[Tensor[L, 4]]): the box coordinates in (x1, y1, x2, y2)\n",
        "            format where the regions will be taken from.\n",
        "            The coordinate must satisfy ``0 <= x1 < x2`` and ``0 <= y1 < y2``.\n",
        "            If a single Tensor is passed,\n",
        "            then the first column should contain the batch index. If a list of Tensors\n",
        "            is passed, then each Tensor will correspond to the boxes for an element i\n",
        "            in a batch\n",
        "        output_size (int or Tuple[int, int]): the size of the output after the cropping\n",
        "            is performed, as (height, width)\n",
        "        spatial_scale (float): a scaling factor that maps the input coordinates to\n",
        "            the box coordinates. Default: 1.0\n",
        "        sampling_ratio (int): number of sampling points in the interpolation grid\n",
        "            used to compute the output value of each pooled output bin. If > 0,\n",
        "            then exactly sampling_ratio x sampling_ratio grid points are used. If\n",
        "            <= 0, then an adaptive number of grid points are used (computed as\n",
        "            ceil(roi_width / pooled_w), and likewise for height). Default: -1\n",
        "        aligned (bool): If False, use the legacy implementation.\n",
        "            If True, pixel shift it by -0.5 for align more perfectly about two neighboring pixel indices.\n",
        "            This version in Detectron2\n",
        "    Returns:\n",
        "        Tensor[K, C, output_size[0], output_size[1]]: The pooled RoIs.\n",
        "    \"\"\"\n",
        "    _assert_has_ops()\n",
        "    check_roi_boxes_shape(boxes)\n",
        "    rois = boxes\n",
        "    output_size = _pair(output_size)\n",
        "    if not isinstance(rois, torch.Tensor):\n",
        "        rois = convert_boxes_to_roi_format(rois)\n",
        "    return torch.ops.torchvision.roi_align(input, rois, spatial_scale,\n",
        "                                           output_size[0], output_size[1],\n",
        "                                           sampling_ratio, aligned)\n",
        "\n",
        "\n",
        "class RoIAlign(nn.Module):\n",
        "    \"\"\"\n",
        "    See :func:`roi_align`.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        output_size: BroadcastingList2[int],\n",
        "        spatial_scale: float,\n",
        "        sampling_ratio: int,\n",
        "        aligned: bool = False,\n",
        "    ):\n",
        "        super(RoIAlign, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.spatial_scale = spatial_scale\n",
        "        self.sampling_ratio = sampling_ratio\n",
        "        self.aligned = aligned\n",
        "\n",
        "    def forward(self, input: Tensor, rois: Tensor) -> Tensor:\n",
        "        return roi_align(input, rois, self.output_size, self.spatial_scale, self.sampling_ratio, self.aligned)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        tmpstr = self.__class__.__name__ + '('\n",
        "        tmpstr += 'output_size=' + str(self.output_size)\n",
        "        tmpstr += ', spatial_scale=' + str(self.spatial_scale)\n",
        "        tmpstr += ', sampling_ratio=' + str(self.sampling_ratio)\n",
        "        tmpstr += ', aligned=' + str(self.aligned)\n",
        "        tmpstr += ')'\n",
        "        return tmpstr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSqVwKGr--wc"
      },
      "source": [
        "#### 2. RPN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBQtbUX__CD_"
      },
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch import nn, Tensor\n",
        "\n",
        "import torchvision\n",
        "from torchvision.ops import boxes as box_ops\n",
        "\n",
        "from . import _utils as det_utils\n",
        "from .image_list import ImageList\n",
        "\n",
        "from typing import List, Optional, Dict, Tuple\n",
        "\n",
        "# Import AnchorGenerator to keep compatibility.\n",
        "from .anchor_utils import AnchorGenerator\n",
        "\n",
        "\n",
        "@torch.jit.unused\n",
        "def _onnx_get_num_anchors_and_pre_nms_top_n(ob, orig_pre_nms_top_n):\n",
        "    # type: (Tensor, int) -> Tuple[int, int]\n",
        "    from torch.onnx import operators\n",
        "    num_anchors = operators.shape_as_tensor(ob)[1].unsqueeze(0)\n",
        "    pre_nms_top_n = torch.min(torch.cat(\n",
        "        (torch.tensor([orig_pre_nms_top_n], dtype=num_anchors.dtype),\n",
        "         num_anchors), 0))\n",
        "\n",
        "    return num_anchors, pre_nms_top_n\n",
        "\n",
        "\n",
        "class RPNHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Adds a simple RPN Head with classification and regression heads\n",
        "    Args:\n",
        "        in_channels (int): number of channels of the input feature\n",
        "        num_anchors (int): number of anchors to be predicted\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, num_anchors):\n",
        "        super(RPNHead, self).__init__()\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels, in_channels, kernel_size=3, stride=1, padding=1\n",
        "        )\n",
        "        self.cls_logits = nn.Conv2d(in_channels, num_anchors, kernel_size=1, stride=1)\n",
        "        self.bbox_pred = nn.Conv2d(\n",
        "            in_channels, num_anchors * 4, kernel_size=1, stride=1\n",
        "        )\n",
        "\n",
        "        for layer in self.children():\n",
        "            torch.nn.init.normal_(layer.weight, std=0.01)\n",
        "            torch.nn.init.constant_(layer.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # type: (List[Tensor]) -> Tuple[List[Tensor], List[Tensor]]\n",
        "        logits = []\n",
        "        bbox_reg = []\n",
        "        for feature in x:\n",
        "            t = F.relu(self.conv(feature))\n",
        "            logits.append(self.cls_logits(t))\n",
        "            bbox_reg.append(self.bbox_pred(t))\n",
        "        return logits, bbox_reg\n",
        "\n",
        "\n",
        "def permute_and_flatten(layer, N, A, C, H, W):\n",
        "    # type: (Tensor, int, int, int, int, int) -> Tensor\n",
        "    layer = layer.view(N, -1, C, H, W)\n",
        "    layer = layer.permute(0, 3, 4, 1, 2)\n",
        "    layer = layer.reshape(N, -1, C)\n",
        "    return layer\n",
        "\n",
        "\n",
        "def concat_box_prediction_layers(box_cls, box_regression):\n",
        "    # type: (List[Tensor], List[Tensor]) -> Tuple[Tensor, Tensor]\n",
        "    box_cls_flattened = []\n",
        "    box_regression_flattened = []\n",
        "    # for each feature level, permute the outputs to make them be in the\n",
        "    # same format as the labels. Note that the labels are computed for\n",
        "    # all feature levels concatenated, so we keep the same representation\n",
        "    # for the objectness and the box_regression\n",
        "    for box_cls_per_level, box_regression_per_level in zip(\n",
        "        box_cls, box_regression\n",
        "    ):\n",
        "        N, AxC, H, W = box_cls_per_level.shape\n",
        "        Ax4 = box_regression_per_level.shape[1]\n",
        "        A = Ax4 // 4\n",
        "        C = AxC // A\n",
        "        box_cls_per_level = permute_and_flatten(\n",
        "            box_cls_per_level, N, A, C, H, W\n",
        "        )\n",
        "        box_cls_flattened.append(box_cls_per_level)\n",
        "\n",
        "        box_regression_per_level = permute_and_flatten(\n",
        "            box_regression_per_level, N, A, 4, H, W\n",
        "        )\n",
        "        box_regression_flattened.append(box_regression_per_level)\n",
        "    # concatenate on the first dimension (representing the feature levels), to\n",
        "    # take into account the way the labels were generated (with all feature maps\n",
        "    # being concatenated as well)\n",
        "    box_cls = torch.cat(box_cls_flattened, dim=1).flatten(0, -2)\n",
        "    box_regression = torch.cat(box_regression_flattened, dim=1).reshape(-1, 4)\n",
        "    return box_cls, box_regression\n",
        "\n",
        "\n",
        "class RegionProposalNetwork(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Implements Region Proposal Network (RPN).\n",
        "    Args:\n",
        "        anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature\n",
        "            maps.\n",
        "        head (nn.Module): module that computes the objectness and regression deltas\n",
        "        fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be\n",
        "            considered as positive during training of the RPN.\n",
        "        bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be\n",
        "            considered as negative during training of the RPN.\n",
        "        batch_size_per_image (int): number of anchors that are sampled during training of the RPN\n",
        "            for computing the loss\n",
        "        positive_fraction (float): proportion of positive anchors in a mini-batch during training\n",
        "            of the RPN\n",
        "        pre_nms_top_n (Dict[int]): number of proposals to keep before applying NMS. It should\n",
        "            contain two fields: training and testing, to allow for different values depending\n",
        "            on training or evaluation\n",
        "        post_nms_top_n (Dict[int]): number of proposals to keep after applying NMS. It should\n",
        "            contain two fields: training and testing, to allow for different values depending\n",
        "            on training or evaluation\n",
        "        nms_thresh (float): NMS threshold used for postprocessing the RPN proposals\n",
        "    \"\"\"\n",
        "    __annotations__ = {\n",
        "        'box_coder': det_utils.BoxCoder,\n",
        "        'proposal_matcher': det_utils.Matcher,\n",
        "        'fg_bg_sampler': det_utils.BalancedPositiveNegativeSampler,\n",
        "        'pre_nms_top_n': Dict[str, int],\n",
        "        'post_nms_top_n': Dict[str, int],\n",
        "    }\n",
        "\n",
        "    def __init__(self,\n",
        "                 anchor_generator,\n",
        "                 head,\n",
        "                 #\n",
        "                 fg_iou_thresh, bg_iou_thresh,\n",
        "                 batch_size_per_image, positive_fraction,\n",
        "                 #\n",
        "                 pre_nms_top_n, post_nms_top_n, nms_thresh, score_thresh=0.0):\n",
        "        super(RegionProposalNetwork, self).__init__()\n",
        "        self.anchor_generator = anchor_generator\n",
        "        self.head = head\n",
        "        self.box_coder = det_utils.BoxCoder(weights=(1.0, 1.0, 1.0, 1.0))\n",
        "\n",
        "        # used during training\n",
        "        self.box_similarity = box_ops.box_iou\n",
        "\n",
        "        self.proposal_matcher = det_utils.Matcher(\n",
        "            fg_iou_thresh,\n",
        "            bg_iou_thresh,\n",
        "            allow_low_quality_matches=True,\n",
        "        )\n",
        "\n",
        "        self.fg_bg_sampler = det_utils.BalancedPositiveNegativeSampler(\n",
        "            batch_size_per_image, positive_fraction\n",
        "        )\n",
        "        # used during testing\n",
        "        self._pre_nms_top_n = pre_nms_top_n\n",
        "        self._post_nms_top_n = post_nms_top_n\n",
        "        self.nms_thresh = nms_thresh\n",
        "        self.score_thresh = score_thresh\n",
        "        self.min_size = 1e-3\n",
        "\n",
        "    def pre_nms_top_n(self):\n",
        "        if self.training:\n",
        "            return self._pre_nms_top_n['training']\n",
        "        return self._pre_nms_top_n['testing']\n",
        "\n",
        "    def post_nms_top_n(self):\n",
        "        if self.training:\n",
        "            return self._post_nms_top_n['training']\n",
        "        return self._post_nms_top_n['testing']\n",
        "\n",
        "    def assign_targets_to_anchors(self, anchors, targets):\n",
        "        # type: (List[Tensor], List[Dict[str, Tensor]]) -> Tuple[List[Tensor], List[Tensor]]\n",
        "        labels = []\n",
        "        matched_gt_boxes = []\n",
        "        for anchors_per_image, targets_per_image in zip(anchors, targets):\n",
        "            gt_boxes = targets_per_image[\"boxes\"]\n",
        "\n",
        "            if gt_boxes.numel() == 0:\n",
        "                # Background image (negative example)\n",
        "                device = anchors_per_image.device\n",
        "                matched_gt_boxes_per_image = torch.zeros(anchors_per_image.shape, dtype=torch.float32, device=device)\n",
        "                labels_per_image = torch.zeros((anchors_per_image.shape[0],), dtype=torch.float32, device=device)\n",
        "            else:\n",
        "                match_quality_matrix = self.box_similarity(gt_boxes, anchors_per_image)\n",
        "                matched_idxs = self.proposal_matcher(match_quality_matrix)\n",
        "                # get the targets corresponding GT for each proposal\n",
        "                # NB: need to clamp the indices because we can have a single\n",
        "                # GT in the image, and matched_idxs can be -2, which goes\n",
        "                # out of bounds\n",
        "                matched_gt_boxes_per_image = gt_boxes[matched_idxs.clamp(min=0)]\n",
        "\n",
        "                labels_per_image = matched_idxs >= 0\n",
        "                labels_per_image = labels_per_image.to(dtype=torch.float32)\n",
        "\n",
        "                # Background (negative examples)\n",
        "                bg_indices = matched_idxs == self.proposal_matcher.BELOW_LOW_THRESHOLD\n",
        "                labels_per_image[bg_indices] = 0.0\n",
        "\n",
        "                # discard indices that are between thresholds\n",
        "                inds_to_discard = matched_idxs == self.proposal_matcher.BETWEEN_THRESHOLDS\n",
        "                labels_per_image[inds_to_discard] = -1.0\n",
        "\n",
        "            labels.append(labels_per_image)\n",
        "            matched_gt_boxes.append(matched_gt_boxes_per_image)\n",
        "        return labels, matched_gt_boxes\n",
        "\n",
        "    def _get_top_n_idx(self, objectness, num_anchors_per_level):\n",
        "        # type: (Tensor, List[int]) -> Tensor\n",
        "        r = []\n",
        "        offset = 0\n",
        "        for ob in objectness.split(num_anchors_per_level, 1):\n",
        "            if torchvision._is_tracing():\n",
        "                num_anchors, pre_nms_top_n = _onnx_get_num_anchors_and_pre_nms_top_n(ob, self.pre_nms_top_n())\n",
        "            else:\n",
        "                num_anchors = ob.shape[1]\n",
        "                pre_nms_top_n = min(self.pre_nms_top_n(), num_anchors)\n",
        "            _, top_n_idx = ob.topk(pre_nms_top_n, dim=1)\n",
        "            r.append(top_n_idx + offset)\n",
        "            offset += num_anchors\n",
        "        return torch.cat(r, dim=1)\n",
        "\n",
        "    def filter_proposals(self, proposals, objectness, image_shapes, num_anchors_per_level):\n",
        "        # type: (Tensor, Tensor, List[Tuple[int, int]], List[int]) -> Tuple[List[Tensor], List[Tensor]]\n",
        "        num_images = proposals.shape[0]\n",
        "        device = proposals.device\n",
        "        # do not backprop throught objectness\n",
        "        objectness = objectness.detach()\n",
        "        objectness = objectness.reshape(num_images, -1)\n",
        "\n",
        "        levels = [\n",
        "            torch.full((n,), idx, dtype=torch.int64, device=device)\n",
        "            for idx, n in enumerate(num_anchors_per_level)\n",
        "        ]\n",
        "        levels = torch.cat(levels, 0)\n",
        "        levels = levels.reshape(1, -1).expand_as(objectness)\n",
        "\n",
        "        # select top_n boxes independently per level before applying nms\n",
        "        top_n_idx = self._get_top_n_idx(objectness, num_anchors_per_level)\n",
        "\n",
        "        image_range = torch.arange(num_images, device=device)\n",
        "        batch_idx = image_range[:, None]\n",
        "\n",
        "        objectness = objectness[batch_idx, top_n_idx]\n",
        "        levels = levels[batch_idx, top_n_idx]\n",
        "        proposals = proposals[batch_idx, top_n_idx]\n",
        "\n",
        "        objectness_prob = torch.sigmoid(objectness)\n",
        "\n",
        "        final_boxes = []\n",
        "        final_scores = []\n",
        "        for boxes, scores, lvl, img_shape in zip(proposals, objectness_prob, levels, image_shapes):\n",
        "            boxes = box_ops.clip_boxes_to_image(boxes, img_shape)\n",
        "\n",
        "            # remove small boxes\n",
        "            keep = box_ops.remove_small_boxes(boxes, self.min_size)\n",
        "            boxes, scores, lvl = boxes[keep], scores[keep], lvl[keep]\n",
        "\n",
        "            # remove low scoring boxes\n",
        "            # use >= for Backwards compatibility\n",
        "            keep = torch.where(scores >= self.score_thresh)[0]\n",
        "            boxes, scores, lvl = boxes[keep], scores[keep], lvl[keep]\n",
        "\n",
        "            # non-maximum suppression, independently done per level\n",
        "            keep = box_ops.batched_nms(boxes, scores, lvl, self.nms_thresh)\n",
        "\n",
        "            # keep only topk scoring predictions\n",
        "            keep = keep[:self.post_nms_top_n()]\n",
        "            boxes, scores = boxes[keep], scores[keep]\n",
        "\n",
        "            final_boxes.append(boxes)\n",
        "            final_scores.append(scores)\n",
        "        return final_boxes, final_scores\n",
        "\n",
        "    def compute_loss(self, objectness, pred_bbox_deltas, labels, regression_targets):\n",
        "        # type: (Tensor, Tensor, List[Tensor], List[Tensor]) -> Tuple[Tensor, Tensor]\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            objectness (Tensor)\n",
        "            pred_bbox_deltas (Tensor)\n",
        "            labels (List[Tensor])\n",
        "            regression_targets (List[Tensor])\n",
        "        Returns:\n",
        "            objectness_loss (Tensor)\n",
        "            box_loss (Tensor)\n",
        "        \"\"\"\n",
        "\n",
        "        sampled_pos_inds, sampled_neg_inds = self.fg_bg_sampler(labels)\n",
        "        sampled_pos_inds = torch.where(torch.cat(sampled_pos_inds, dim=0))[0]\n",
        "        sampled_neg_inds = torch.where(torch.cat(sampled_neg_inds, dim=0))[0]\n",
        "\n",
        "        sampled_inds = torch.cat([sampled_pos_inds, sampled_neg_inds], dim=0)\n",
        "\n",
        "        objectness = objectness.flatten()\n",
        "\n",
        "        labels = torch.cat(labels, dim=0)\n",
        "        regression_targets = torch.cat(regression_targets, dim=0)\n",
        "\n",
        "        box_loss = F.smooth_l1_loss(\n",
        "            pred_bbox_deltas[sampled_pos_inds],\n",
        "            regression_targets[sampled_pos_inds],\n",
        "            beta=1 / 9,\n",
        "            reduction='sum',\n",
        "        ) / (sampled_inds.numel())\n",
        "\n",
        "        objectness_loss = F.binary_cross_entropy_with_logits(\n",
        "            objectness[sampled_inds], labels[sampled_inds]\n",
        "        )\n",
        "\n",
        "        return objectness_loss, box_loss\n",
        "\n",
        "    def forward(self,\n",
        "                images,       # type: ImageList\n",
        "                features,     # type: Dict[str, Tensor]\n",
        "                targets=None  # type: Optional[List[Dict[str, Tensor]]]\n",
        "                ):\n",
        "        # type: (...) -> Tuple[List[Tensor], Dict[str, Tensor]]\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images (ImageList): images for which we want to compute the predictions\n",
        "            features (OrderedDict[Tensor]): features computed from the images that are\n",
        "                used for computing the predictions. Each tensor in the list\n",
        "                correspond to different feature levels\n",
        "            targets (List[Dict[Tensor]]): ground-truth boxes present in the image (optional).\n",
        "                If provided, each element in the dict should contain a field `boxes`,\n",
        "                with the locations of the ground-truth boxes.\n",
        "        Returns:\n",
        "            boxes (List[Tensor]): the predicted boxes from the RPN, one Tensor per\n",
        "                image.\n",
        "            losses (Dict[Tensor]): the losses for the model during training. During\n",
        "                testing, it is an empty dict.\n",
        "        \"\"\"\n",
        "        # RPN uses all feature maps that are available\n",
        "        features = list(features.values())\n",
        "        objectness, pred_bbox_deltas = self.head(features)\n",
        "        anchors = self.anchor_generator(images, features)\n",
        "\n",
        "        num_images = len(anchors)\n",
        "        num_anchors_per_level_shape_tensors = [o[0].shape for o in objectness]\n",
        "        num_anchors_per_level = [s[0] * s[1] * s[2] for s in num_anchors_per_level_shape_tensors]\n",
        "        objectness, pred_bbox_deltas = \\\n",
        "            concat_box_prediction_layers(objectness, pred_bbox_deltas)\n",
        "        # apply pred_bbox_deltas to anchors to obtain the decoded proposals\n",
        "        # note that we detach the deltas because Faster R-CNN do not backprop through\n",
        "        # the proposals\n",
        "        proposals = self.box_coder.decode(pred_bbox_deltas.detach(), anchors)\n",
        "        proposals = proposals.view(num_images, -1, 4)\n",
        "        boxes, scores = self.filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)\n",
        "\n",
        "        losses = {}\n",
        "        if self.training:\n",
        "            assert targets is not None\n",
        "            labels, matched_gt_boxes = self.assign_targets_to_anchors(anchors, targets)\n",
        "            regression_targets = self.box_coder.encode(matched_gt_boxes, anchors)\n",
        "            loss_objectness, loss_rpn_box_reg = self.compute_loss(\n",
        "                objectness, pred_bbox_deltas, labels, regression_targets)\n",
        "            losses = {\n",
        "                \"loss_objectness\": loss_objectness,\n",
        "                \"loss_rpn_box_reg\": loss_rpn_box_reg,\n",
        "            }\n",
        "        return boxes, losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lom16-oo_XR0"
      },
      "source": [
        "#### 3. Backbone with FPN (Feature Pyramid Network)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLb8TLPH_lpT"
      },
      "source": [
        "class BackboneWithFPN(nn.Module):\n",
        "    \"\"\"\n",
        "    Adds a FPN on top of a model.\n",
        "    Internally, it uses torchvision.models._utils.IntermediateLayerGetter to\n",
        "    extract a submodel that returns the feature maps specified in return_layers.\n",
        "    The same limitations of IntermediatLayerGetter apply here.\n",
        "    Args:\n",
        "        backbone (nn.Module)\n",
        "        return_layers (Dict[name, new_name]): a dict containing the names\n",
        "            of the modules for which the activations will be returned as\n",
        "            the key of the dict, and the value of the dict is the name\n",
        "            of the returned activation (which the user can specify).\n",
        "        in_channels_list (List[int]): number of channels for each feature map\n",
        "            that is returned, in the order they are present in the OrderedDict\n",
        "        out_channels (int): number of channels in the FPN.\n",
        "    Attributes:\n",
        "        out_channels (int): the number of channels in the FPN\n",
        "    \"\"\"\n",
        "    def __init__(self, backbone, return_layers, in_channels_list, out_channels, extra_blocks=None):\n",
        "        super(BackboneWithFPN, self).__init__()\n",
        "\n",
        "        if extra_blocks is None:\n",
        "            extra_blocks = LastLevelMaxPool()\n",
        "\n",
        "        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
        "        self.fpn = FeaturePyramidNetwork(\n",
        "            in_channels_list=in_channels_list,\n",
        "            out_channels=out_channels,\n",
        "            extra_blocks=extra_blocks,\n",
        "        )\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.body(x)\n",
        "        x = self.fpn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def resnet_fpn_backbone(\n",
        "    backbone_name,\n",
        "    pretrained,\n",
        "    norm_layer=misc_nn_ops.FrozenBatchNorm2d,\n",
        "    trainable_layers=3,\n",
        "    returned_layers=None,\n",
        "    extra_blocks=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Constructs a specified ResNet backbone with FPN on top. Freezes the specified number of layers in the backbone.\n",
        "    Examples::\n",
        "        >>> from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "        >>> backbone = resnet_fpn_backbone('resnet50', pretrained=True, trainable_layers=3)\n",
        "        >>> # get some dummy image\n",
        "        >>> x = torch.rand(1,3,64,64)\n",
        "        >>> # compute the output\n",
        "        >>> output = backbone(x)\n",
        "        >>> print([(k, v.shape) for k, v in output.items()])\n",
        "        >>> # returns\n",
        "        >>>   [('0', torch.Size([1, 256, 16, 16])),\n",
        "        >>>    ('1', torch.Size([1, 256, 8, 8])),\n",
        "        >>>    ('2', torch.Size([1, 256, 4, 4])),\n",
        "        >>>    ('3', torch.Size([1, 256, 2, 2])),\n",
        "        >>>    ('pool', torch.Size([1, 256, 1, 1]))]\n",
        "    Args:\n",
        "        backbone_name (string): resnet architecture. Possible values are 'ResNet', 'resnet18', 'resnet34', 'resnet50',\n",
        "             'resnet101', 'resnet152', 'resnext50_32x4d', 'resnext101_32x8d', 'wide_resnet50_2', 'wide_resnet101_2'\n",
        "        pretrained (bool): If True, returns a model with backbone pre-trained on Imagenet\n",
        "        norm_layer (torchvision.ops): it is recommended to use the default value. For details visit:\n",
        "            (https://github.com/facebookresearch/maskrcnn-benchmark/issues/267)\n",
        "        trainable_layers (int): number of trainable (not frozen) resnet layers starting from final block.\n",
        "            Valid values are between 0 and 5, with 5 meaning all backbone layers are trainable.\n",
        "        returned_layers (list of int): The layers of the network to return. Each entry must be in ``[1, 4]``.\n",
        "            By default all layers are returned.\n",
        "        extra_blocks (ExtraFPNBlock or None): if provided, extra operations will\n",
        "            be performed. It is expected to take the fpn features, the original\n",
        "            features and the names of the original features as input, and returns\n",
        "            a new list of feature maps and their corresponding names. By\n",
        "            default a ``LastLevelMaxPool`` is used.\n",
        "    \"\"\"\n",
        "    backbone = resnet.__dict__[backbone_name](\n",
        "        pretrained=pretrained,\n",
        "        norm_layer=norm_layer)\n",
        "\n",
        "    # select layers that wont be frozen\n",
        "    assert 0 <= trainable_layers <= 5\n",
        "    layers_to_train = ['layer4', 'layer3', 'layer2', 'layer1', 'conv1'][:trainable_layers]\n",
        "    if trainable_layers == 5:\n",
        "        layers_to_train.append('bn1')\n",
        "    for name, parameter in backbone.named_parameters():\n",
        "        if all([not name.startswith(layer) for layer in layers_to_train]):\n",
        "            parameter.requires_grad_(False)\n",
        "\n",
        "    if extra_blocks is None:\n",
        "        extra_blocks = LastLevelMaxPool()\n",
        "\n",
        "    if returned_layers is None:\n",
        "        returned_layers = [1, 2, 3, 4]\n",
        "    assert min(returned_layers) > 0 and max(returned_layers) < 5\n",
        "    return_layers = {f'layer{k}': str(v) for v, k in enumerate(returned_layers)}\n",
        "\n",
        "    in_channels_stage2 = backbone.inplanes // 8\n",
        "    in_channels_list = [in_channels_stage2 * 2 ** (i - 1) for i in returned_layers]\n",
        "    out_channels = 256\n",
        "    return BackboneWithFPN(backbone, return_layers, in_channels_list, out_channels, extra_blocks=extra_blocks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4655kDTlA-dY"
      },
      "source": [
        "#### 4. FPN (Feature Pyramid Network)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOz1pVRqBDyi"
      },
      "source": [
        "class FeaturePyramidNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Module that adds a FPN from on top of a set of feature maps. This is based on\n",
        "    `\"Feature Pyramid Network for Object Detection\" <https://arxiv.org/abs/1612.03144>`_.\n",
        "    The feature maps are currently supposed to be in increasing depth\n",
        "    order.\n",
        "    The input to the model is expected to be an OrderedDict[Tensor], containing\n",
        "    the feature maps on top of which the FPN will be added.\n",
        "    Args:\n",
        "        in_channels_list (list[int]): number of channels for each feature map that\n",
        "            is passed to the module\n",
        "        out_channels (int): number of channels of the FPN representation\n",
        "        extra_blocks (ExtraFPNBlock or None): if provided, extra operations will\n",
        "            be performed. It is expected to take the fpn features, the original\n",
        "            features and the names of the original features as input, and returns\n",
        "            a new list of feature maps and their corresponding names\n",
        "    Examples::\n",
        "        >>> m = torchvision.ops.FeaturePyramidNetwork([10, 20, 30], 5)\n",
        "        >>> # get some dummy data\n",
        "        >>> x = OrderedDict()\n",
        "        >>> x['feat0'] = torch.rand(1, 10, 64, 64)\n",
        "        >>> x['feat2'] = torch.rand(1, 20, 16, 16)\n",
        "        >>> x['feat3'] = torch.rand(1, 30, 8, 8)\n",
        "        >>> # compute the FPN on top of x\n",
        "        >>> output = m(x)\n",
        "        >>> print([(k, v.shape) for k, v in output.items()])\n",
        "        >>> # returns\n",
        "        >>>   [('feat0', torch.Size([1, 5, 64, 64])),\n",
        "        >>>    ('feat2', torch.Size([1, 5, 16, 16])),\n",
        "        >>>    ('feat3', torch.Size([1, 5, 8, 8]))]\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels_list: List[int],\n",
        "        out_channels: int,\n",
        "        extra_blocks: Optional[ExtraFPNBlock] = None,\n",
        "    ):\n",
        "        super(FeaturePyramidNetwork, self).__init__()\n",
        "        self.inner_blocks = nn.ModuleList()\n",
        "        self.layer_blocks = nn.ModuleList()\n",
        "        for in_channels in in_channels_list:\n",
        "            if in_channels == 0:\n",
        "                raise ValueError(\"in_channels=0 is currently not supported\")\n",
        "            inner_block_module = nn.Conv2d(in_channels, out_channels, 1)\n",
        "            layer_block_module = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "            self.inner_blocks.append(inner_block_module)\n",
        "            self.layer_blocks.append(layer_block_module)\n",
        "\n",
        "        # initialize parameters now to avoid modifying the initialization of top_blocks\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_uniform_(m.weight, a=1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        if extra_blocks is not None:\n",
        "            assert isinstance(extra_blocks, ExtraFPNBlock)\n",
        "        self.extra_blocks = extra_blocks\n",
        "\n",
        "    def get_result_from_inner_blocks(self, x: Tensor, idx: int) -> Tensor:\n",
        "        \"\"\"\n",
        "        This is equivalent to self.inner_blocks[idx](x),\n",
        "        but torchscript doesn't support this yet\n",
        "        \"\"\"\n",
        "        num_blocks = len(self.inner_blocks)\n",
        "        if idx < 0:\n",
        "            idx += num_blocks\n",
        "        i = 0\n",
        "        out = x\n",
        "        for module in self.inner_blocks:\n",
        "            if i == idx:\n",
        "                out = module(x)\n",
        "            i += 1\n",
        "        return out\n",
        "\n",
        "    def get_result_from_layer_blocks(self, x: Tensor, idx: int) -> Tensor:\n",
        "        \"\"\"\n",
        "        This is equivalent to self.layer_blocks[idx](x),\n",
        "        but torchscript doesn't support this yet\n",
        "        \"\"\"\n",
        "        num_blocks = len(self.layer_blocks)\n",
        "        if idx < 0:\n",
        "            idx += num_blocks\n",
        "        i = 0\n",
        "        out = x\n",
        "        for module in self.layer_blocks:\n",
        "            if i == idx:\n",
        "                out = module(x)\n",
        "            i += 1\n",
        "        return out\n",
        "\n",
        "    def forward(self, x: Dict[str, Tensor]) -> Dict[str, Tensor]:\n",
        "        \"\"\"\n",
        "        Computes the FPN for a set of feature maps.\n",
        "        Args:\n",
        "            x (OrderedDict[Tensor]): feature maps for each feature level.\n",
        "        Returns:\n",
        "            results (OrderedDict[Tensor]): feature maps after FPN layers.\n",
        "                They are ordered from highest resolution first.\n",
        "        \"\"\"\n",
        "        # unpack OrderedDict into two lists for easier handling\n",
        "        names = list(x.keys())\n",
        "        x = list(x.values())\n",
        "\n",
        "        last_inner = self.get_result_from_inner_blocks(x[-1], -1)\n",
        "        results = []\n",
        "        results.append(self.get_result_from_layer_blocks(last_inner, -1))\n",
        "\n",
        "        for idx in range(len(x) - 2, -1, -1):\n",
        "            inner_lateral = self.get_result_from_inner_blocks(x[idx], idx)\n",
        "            feat_shape = inner_lateral.shape[-2:]\n",
        "            inner_top_down = F.interpolate(last_inner, size=feat_shape, mode=\"nearest\")\n",
        "            last_inner = inner_lateral + inner_top_down\n",
        "            results.insert(0, self.get_result_from_layer_blocks(last_inner, idx))\n",
        "\n",
        "        if self.extra_blocks is not None:\n",
        "            results, names = self.extra_blocks(results, x, names)\n",
        "\n",
        "        # make it back an OrderedDict\n",
        "        out = OrderedDict([(k, v) for k, v in zip(names, results)])\n",
        "\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GozU4wynf5Xt"
      },
      "source": [
        "### 1. Mask RCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y18q4nsxfcZH"
      },
      "source": [
        "class MaskRCNN(FasterRCNN):\n",
        "    \"\"\"\n",
        "    Implements Mask R-CNN.\n",
        "    The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each\n",
        "    image, and should be in 0-1 range. Different images can have different sizes.\n",
        "    The behavior of the model changes depending if it is in training or evaluation mode.\n",
        "    During training, the model expects both the input tensors, as well as a targets (list of dictionary),\n",
        "    containing:\n",
        "        - boxes (FloatTensor[N, 4]): the ground-truth boxes in [x1, y1, x2, y2] format, with values of x\n",
        "          between 0 and W and values of y between 0 and H\n",
        "        - labels (Int64Tensor[N]): the class label for each ground-truth box\n",
        "        - masks (UInt8Tensor[N, H, W]): the segmentation binary masks for each instance\n",
        "    The model returns a Dict[Tensor] during training, containing the classification and regression\n",
        "    losses for both the RPN and the R-CNN, and the mask loss.\n",
        "    During inference, the model requires only the input tensors, and returns the post-processed\n",
        "    predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as\n",
        "    follows:\n",
        "        - boxes (FloatTensor[N, 4]): the predicted boxes in [x1, y1, x2, y2] format, with values of x\n",
        "          between 0 and W and values of y between 0 and H\n",
        "        - labels (Int64Tensor[N]): the predicted labels for each image\n",
        "        - scores (Tensor[N]): the scores or each prediction\n",
        "        - masks (UInt8Tensor[N, 1, H, W]): the predicted masks for each instance, in 0-1 range. In order to\n",
        "          obtain the final segmentation masks, the soft masks can be thresholded, generally\n",
        "          with a value of 0.5 (mask >= 0.5)\n",
        "    Args:\n",
        "        backbone (nn.Module): the network used to compute the features for the model.\n",
        "            It should contain a out_channels attribute, which indicates the number of output\n",
        "            channels that each feature map has (and it should be the same for all feature maps).\n",
        "            The backbone should return a single Tensor or and OrderedDict[Tensor].\n",
        "        num_classes (int): number of output classes of the model (including the background).\n",
        "            If box_predictor is specified, num_classes should be None.\n",
        "        min_size (int): minimum size of the image to be rescaled before feeding it to the backbone\n",
        "        max_size (int): maximum size of the image to be rescaled before feeding it to the backbone\n",
        "        image_mean (Tuple[float, float, float]): mean values used for input normalization.\n",
        "            They are generally the mean values of the dataset on which the backbone has been trained\n",
        "            on\n",
        "        image_std (Tuple[float, float, float]): std values used for input normalization.\n",
        "            They are generally the std values of the dataset on which the backbone has been trained on\n",
        "        rpn_anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature\n",
        "            maps.\n",
        "        rpn_head (nn.Module): module that computes the objectness and regression deltas from the RPN\n",
        "        rpn_pre_nms_top_n_train (int): number of proposals to keep before applying NMS during training\n",
        "        rpn_pre_nms_top_n_test (int): number of proposals to keep before applying NMS during testing\n",
        "        rpn_post_nms_top_n_train (int): number of proposals to keep after applying NMS during training\n",
        "        rpn_post_nms_top_n_test (int): number of proposals to keep after applying NMS during testing\n",
        "        rpn_nms_thresh (float): NMS threshold used for postprocessing the RPN proposals\n",
        "        rpn_fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be\n",
        "            considered as positive during training of the RPN.\n",
        "        rpn_bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be\n",
        "            considered as negative during training of the RPN.\n",
        "        rpn_batch_size_per_image (int): number of anchors that are sampled during training of the RPN\n",
        "            for computing the loss\n",
        "        rpn_positive_fraction (float): proportion of positive anchors in a mini-batch during training\n",
        "            of the RPN\n",
        "        rpn_score_thresh (float): during inference, only return proposals with a classification score\n",
        "            greater than rpn_score_thresh\n",
        "        box_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in\n",
        "            the locations indicated by the bounding boxes\n",
        "        box_head (nn.Module): module that takes the cropped feature maps as input\n",
        "        box_predictor (nn.Module): module that takes the output of box_head and returns the\n",
        "            classification logits and box regression deltas.\n",
        "        box_score_thresh (float): during inference, only return proposals with a classification score\n",
        "            greater than box_score_thresh\n",
        "        box_nms_thresh (float): NMS threshold for the prediction head. Used during inference\n",
        "        box_detections_per_img (int): maximum number of detections per image, for all classes.\n",
        "        box_fg_iou_thresh (float): minimum IoU between the proposals and the GT box so that they can be\n",
        "            considered as positive during training of the classification head\n",
        "        box_bg_iou_thresh (float): maximum IoU between the proposals and the GT box so that they can be\n",
        "            considered as negative during training of the classification head\n",
        "        box_batch_size_per_image (int): number of proposals that are sampled during training of the\n",
        "            classification head\n",
        "        box_positive_fraction (float): proportion of positive proposals in a mini-batch during training\n",
        "            of the classification head\n",
        "        bbox_reg_weights (Tuple[float, float, float, float]): weights for the encoding/decoding of the\n",
        "            bounding boxes\n",
        "        mask_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in\n",
        "             the locations indicated by the bounding boxes, which will be used for the mask head.\n",
        "        mask_head (nn.Module): module that takes the cropped feature maps as input\n",
        "        mask_predictor (nn.Module): module that takes the output of the mask_head and returns the\n",
        "            segmentation mask logits\n",
        "    Example::\n",
        "        >>> import torch\n",
        "        >>> import torchvision\n",
        "        >>> from torchvision.models.detection import MaskRCNN\n",
        "        >>> from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
        "        >>>\n",
        "        >>> # load a pre-trained model for classification and return\n",
        "        >>> # only the features\n",
        "        >>> backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "        >>> # MaskRCNN needs to know the number of\n",
        "        >>> # output channels in a backbone. For mobilenet_v2, it's 1280\n",
        "        >>> # so we need to add it here\n",
        "        >>> backbone.out_channels = 1280\n",
        "        >>>\n",
        "        >>> # let's make the RPN generate 5 x 3 anchors per spatial\n",
        "        >>> # location, with 5 different sizes and 3 different aspect\n",
        "        >>> # ratios. We have a Tuple[Tuple[int]] because each feature\n",
        "        >>> # map could potentially have different sizes and\n",
        "        >>> # aspect ratios\n",
        "        >>> anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "        >>>                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "        >>>\n",
        "        >>> # let's define what are the feature maps that we will\n",
        "        >>> # use to perform the region of interest cropping, as well as\n",
        "        >>> # the size of the crop after rescaling.\n",
        "        >>> # if your backbone returns a Tensor, featmap_names is expected to\n",
        "        >>> # be ['0']. More generally, the backbone should return an\n",
        "        >>> # OrderedDict[Tensor], and in featmap_names you can choose which\n",
        "        >>> # feature maps to use.\n",
        "        >>> roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
        "        >>>                                                 output_size=7,\n",
        "        >>>                                                 sampling_ratio=2)\n",
        "        >>>\n",
        "        >>> mask_roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
        "        >>>                                                      output_size=14,\n",
        "        >>>                                                      sampling_ratio=2)\n",
        "        >>> # put the pieces together inside a MaskRCNN model\n",
        "        >>> model = MaskRCNN(backbone,\n",
        "        >>>                  num_classes=2,\n",
        "        >>>                  rpn_anchor_generator=anchor_generator,\n",
        "        >>>                  box_roi_pool=roi_pooler,\n",
        "        >>>                  mask_roi_pool=mask_roi_pooler)\n",
        "        >>> model.eval()\n",
        "        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "        >>> predictions = model(x)\n",
        "    \"\"\"\n",
        "    def __init__(self, backbone, num_classes=None,\n",
        "                 # transform parameters\n",
        "                 min_size=800, max_size=1333,\n",
        "                 image_mean=None, image_std=None,\n",
        "                 # RPN parameters\n",
        "                 rpn_anchor_generator=None, rpn_head=None,\n",
        "                 rpn_pre_nms_top_n_train=2000, rpn_pre_nms_top_n_test=1000,\n",
        "                 rpn_post_nms_top_n_train=2000, rpn_post_nms_top_n_test=1000,\n",
        "                 rpn_nms_thresh=0.7,\n",
        "                 rpn_fg_iou_thresh=0.7, rpn_bg_iou_thresh=0.3,\n",
        "                 rpn_batch_size_per_image=256, rpn_positive_fraction=0.5,\n",
        "                 rpn_score_thresh=0.0,\n",
        "                 # Box parameters\n",
        "                 box_roi_pool=None, box_head=None, box_predictor=None,\n",
        "                 box_score_thresh=0.05, box_nms_thresh=0.5, box_detections_per_img=100,\n",
        "                 box_fg_iou_thresh=0.5, box_bg_iou_thresh=0.5,\n",
        "                 box_batch_size_per_image=512, box_positive_fraction=0.25,\n",
        "                 bbox_reg_weights=None,\n",
        "                 # Mask parameters\n",
        "                 mask_roi_pool=None, mask_head=None, mask_predictor=None):\n",
        "\n",
        "        assert isinstance(mask_roi_pool, (MultiScaleRoIAlign, type(None)))\n",
        "\n",
        "        if num_classes is not None:\n",
        "            if mask_predictor is not None:\n",
        "                raise ValueError(\"num_classes should be None when mask_predictor is specified\")\n",
        "\n",
        "        out_channels = backbone.out_channels\n",
        "\n",
        "        if mask_roi_pool is None:\n",
        "            mask_roi_pool = MultiScaleRoIAlign(\n",
        "                featmap_names=['0', '1', '2', '3'],\n",
        "                output_size=14,\n",
        "                sampling_ratio=2)\n",
        "\n",
        "        if mask_head is None:\n",
        "            mask_layers = (256, 256, 256, 256)\n",
        "            mask_dilation = 1\n",
        "            mask_head = MaskRCNNHeads(out_channels, mask_layers, mask_dilation)\n",
        "\n",
        "        if mask_predictor is None:\n",
        "            mask_predictor_in_channels = 256  # == mask_layers[-1]\n",
        "            mask_dim_reduced = 256\n",
        "            mask_predictor = MaskRCNNPredictor(mask_predictor_in_channels,\n",
        "                                               mask_dim_reduced, num_classes)\n",
        "\n",
        "        super(MaskRCNN, self).__init__(\n",
        "            backbone, num_classes,\n",
        "            # transform parameters\n",
        "            min_size, max_size,\n",
        "            image_mean, image_std,\n",
        "            # RPN-specific parameters\n",
        "            rpn_anchor_generator, rpn_head,\n",
        "            rpn_pre_nms_top_n_train, rpn_pre_nms_top_n_test,\n",
        "            rpn_post_nms_top_n_train, rpn_post_nms_top_n_test,\n",
        "            rpn_nms_thresh,\n",
        "            rpn_fg_iou_thresh, rpn_bg_iou_thresh,\n",
        "            rpn_batch_size_per_image, rpn_positive_fraction,\n",
        "            rpn_score_thresh,\n",
        "            # Box parameters\n",
        "            box_roi_pool, box_head, box_predictor,\n",
        "            box_score_thresh, box_nms_thresh, box_detections_per_img,\n",
        "            box_fg_iou_thresh, box_bg_iou_thresh,\n",
        "            box_batch_size_per_image, box_positive_fraction,\n",
        "            bbox_reg_weights)\n",
        "\n",
        "        self.roi_heads.mask_roi_pool = mask_roi_pool\n",
        "        self.roi_heads.mask_head = mask_head\n",
        "        self.roi_heads.mask_predictor = mask_predictor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzmVtrW-f07j"
      },
      "source": [
        "### 2. Mask RCNN Heads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a544g_omfl-R"
      },
      "source": [
        "class MaskRCNNHeads(nn.Sequential):\n",
        "    def __init__(self, in_channels, layers, dilation):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            in_channels (int): number of input channels\n",
        "            layers (list): feature dimensions of each FCN layer\n",
        "            dilation (int): dilation rate of kernel\n",
        "        \"\"\"\n",
        "        d = OrderedDict()\n",
        "        next_feature = in_channels\n",
        "        for layer_idx, layer_features in enumerate(layers, 1):\n",
        "            d[\"mask_fcn{}\".format(layer_idx)] = nn.Conv2d(\n",
        "                next_feature, layer_features, kernel_size=3,\n",
        "                stride=1, padding=dilation, dilation=dilation)\n",
        "            d[\"relu{}\".format(layer_idx)] = nn.ReLU(inplace=True)\n",
        "            next_feature = layer_features\n",
        "\n",
        "        super(MaskRCNNHeads, self).__init__(d)\n",
        "        for name, param in self.named_parameters():\n",
        "            if \"weight\" in name:\n",
        "                nn.init.kaiming_normal_(param, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            # elif \"bias\" in name:\n",
        "            #     nn.init.constant_(param, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuuvrNXHft6D"
      },
      "source": [
        "### 3. Mask RCNN Predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsd6QxZAfnOF"
      },
      "source": [
        "class MaskRCNNPredictor(nn.Sequential):\n",
        "    def __init__(self, in_channels, dim_reduced, num_classes):\n",
        "        super(MaskRCNNPredictor, self).__init__(OrderedDict([\n",
        "            (\"conv5_mask\", nn.ConvTranspose2d(in_channels, dim_reduced, 2, 2, 0)),\n",
        "            (\"relu\", nn.ReLU(inplace=True)),\n",
        "            (\"mask_fcn_logits\", nn.Conv2d(dim_reduced, num_classes, 1, 1, 0)),\n",
        "        ]))\n",
        "\n",
        "        for name, param in self.named_parameters():\n",
        "            if \"weight\" in name:\n",
        "                nn.init.kaiming_normal_(param, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            # elif \"bias\" in name:\n",
        "            #     nn.init.constant_(param, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x5h9vN3f9jp"
      },
      "source": [
        "### 4. Mask RCNN using Resnet 50 fpn as Backbone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko6HGK5FfKYw"
      },
      "source": [
        "def maskrcnn_resnet50_fpn(pretrained=False, progress=True,\n",
        "                          num_classes=91, pretrained_backbone=True, trainable_backbone_layers=None, **kwargs):\n",
        "    \"\"\"\n",
        "    Constructs a Mask R-CNN model with a ResNet-50-FPN backbone.\n",
        "    The input to the model is expected to be a list of tensors, each of shape ``[C, H, W]``, one for each\n",
        "    image, and should be in ``0-1`` range. Different images can have different sizes.\n",
        "    The behavior of the model changes depending if it is in training or evaluation mode.\n",
        "    During training, the model expects both the input tensors, as well as a targets (list of dictionary),\n",
        "    containing:\n",
        "        - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format,  with values of ``x``\n",
        "          between ``0`` and ``W`` and values of ``y`` between ``0`` and ``H``\n",
        "        - labels (``Int64Tensor[N]``): the class label for each ground-truth box\n",
        "        - masks (``UInt8Tensor[N, H, W]``): the segmentation binary masks for each instance\n",
        "    The model returns a ``Dict[Tensor]`` during training, containing the classification and regression\n",
        "    losses for both the RPN and the R-CNN, and the mask loss.\n",
        "    During inference, the model requires only the input tensors, and returns the post-processed\n",
        "    predictions as a ``List[Dict[Tensor]]``, one for each input image. The fields of the ``Dict`` are as\n",
        "    follows:\n",
        "        - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format,  with values of ``x``\n",
        "          between ``0`` and ``W`` and values of ``y`` between ``0`` and ``H``\n",
        "        - labels (``Int64Tensor[N]``): the predicted labels for each image\n",
        "        - scores (``Tensor[N]``): the scores or each prediction\n",
        "        - masks (``UInt8Tensor[N, 1, H, W]``): the predicted masks for each instance, in ``0-1`` range. In order to\n",
        "          obtain the final segmentation masks, the soft masks can be thresholded, generally\n",
        "          with a value of 0.5 (``mask >= 0.5``)\n",
        "    Mask R-CNN is exportable to ONNX for a fixed batch size with inputs images of fixed size.\n",
        "    Example::\n",
        "        >>> model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "        >>> model.eval()\n",
        "        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "        >>> predictions = model(x)\n",
        "        >>>\n",
        "        >>> # optionally, if you want to export the model to ONNX:\n",
        "        >>> torch.onnx.export(model, x, \"mask_rcnn.onnx\", opset_version = 11)\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on COCO train2017\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "        num_classes (int): number of output classes of the model (including the background)\n",
        "        pretrained_backbone (bool): If True, returns a model with backbone pre-trained on Imagenet\n",
        "        trainable_backbone_layers (int): number of trainable (not frozen) resnet layers starting from final block.\n",
        "            Valid values are between 0 and 5, with 5 meaning all backbone layers are trainable.\n",
        "    \"\"\"\n",
        "    trainable_backbone_layers = _validate_trainable_layers(\n",
        "        pretrained or pretrained_backbone, trainable_backbone_layers, 5, 3)\n",
        "\n",
        "    if pretrained:\n",
        "        # no need to download the backbone if pretrained is set\n",
        "        pretrained_backbone = False\n",
        "    backbone = resnet_fpn_backbone('resnet50', pretrained_backbone, trainable_layers=trainable_backbone_layers)\n",
        "    model = MaskRCNN(backbone, num_classes, **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls['maskrcnn_resnet50_fpn_coco'],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "        overwrite_eps(model, 0.0)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}